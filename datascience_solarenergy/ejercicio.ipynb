{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde: /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/antofagasta_dirty.csv\n",
      "Datos cargados exitosamente.\n",
      "Iniciando limpieza de datos...\n",
      "Limpiando GHI...\n",
      "  Marcados 175 valores > 1250 como NaN.\n",
      "  NaNs interpolados/rellenados (originales: 100, marcados: 175, final: 0).\n",
      "  Corregidos 35 valores < 0 en GHI.\n",
      "Limpiando DNI...\n",
      "  NaNs interpolados/rellenados (originales: 100, marcados: 0, final: 0).\n",
      "  Corregidos 34 valores < 0 en DNI.\n",
      "Limpiando DHI...\n",
      "  NaNs interpolados/rellenados (originales: 100, marcados: 0, final: 0).\n",
      "  Corregidos 35 valores < 0 en DHI.\n",
      "Limpiando Tdry...\n",
      "  NaNs interpolados/rellenados (originales: 135, marcados: 0, final: 0).\n",
      "Limpiando Wspd...\n",
      "  NaNs interpolados/rellenados (originales: 135, marcados: 0, final: 0).\n",
      "  Corregidos 95 valores < 0 en Wspd.\n",
      "Limpieza de datos completada.\n",
      "Creando índice de tiempo (Timestamp)...\n",
      "Índice de tiempo creado y establecido.\n",
      "\n",
      "Información del DataFrame limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 8760 entries, 2004-02-01 00:00:00 to 2015-11-30 23:00:00\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Year        8760 non-null   int64  \n",
      " 1   Month       8760 non-null   int64  \n",
      " 2   Day         8760 non-null   int64  \n",
      " 3   Hour        8760 non-null   int64  \n",
      " 4   Minute      8760 non-null   int64  \n",
      " 5   GHI         8760 non-null   float64\n",
      " 6   DNI         8760 non-null   float64\n",
      " 7   DHI         8760 non-null   float64\n",
      " 8   Tdry        8760 non-null   float64\n",
      " 9   Tdew        8760 non-null   float64\n",
      " 10  RH          8760 non-null   float64\n",
      " 11  Pres        8760 non-null   float64\n",
      " 12  Wspd        8760 non-null   float64\n",
      " 13  Wdir        8760 non-null   float64\n",
      " 14  Snow Depth  8760 non-null   int64  \n",
      "dtypes: float64(9), int64(6)\n",
      "memory usage: 1.1 MB\n",
      "None\n",
      "\n",
      "Primeras filas del DataFrame limpio:\n",
      "                     Year  Month  Day  Hour  Minute  GHI  DNI  DHI  Tdry  \\\n",
      "Timestamp                                                                  \n",
      "2004-02-01 00:00:00  2004      2    1     0       0 -0.0 -0.0 -0.0 -11.5   \n",
      "2004-02-01 01:00:00  2004      2    1     1       0  0.0  0.0  0.0  10.8   \n",
      "2004-02-01 02:00:00  2004      2    1     2       0  0.0  0.0  0.0  10.0   \n",
      "2004-02-01 03:00:00  2004      2    1     3       0  0.0  0.0  0.0   8.7   \n",
      "2004-02-01 04:00:00  2004      2    1     4       0  0.0  0.0  0.0   8.0   \n",
      "\n",
      "                     Tdew    RH   Pres  Wspd  Wdir  Snow Depth  \n",
      "Timestamp                                                       \n",
      "2004-02-01 00:00:00  11.5  40.9  822.5   0.0  22.5           0  \n",
      "2004-02-01 01:00:00  10.8  42.8  820.2   4.8  33.0           0  \n",
      "2004-02-01 02:00:00  10.0  43.7  818.6   5.5  33.0           0  \n",
      "2004-02-01 03:00:00   8.7  59.6  816.1   6.0  27.0           0  \n",
      "2004-02-01 04:00:00   8.0  50.1  815.4   5.7  19.5           0  \n",
      "\n",
      "Últimas filas del DataFrame limpio:\n",
      "                     Year  Month  Day  Hour  Minute   GHI    DNI  DHI  Tdry  \\\n",
      "Timestamp                                                                     \n",
      "2015-11-30 19:00:00  2015     11   30    19       0  14.8  304.3  6.8  19.6   \n",
      "2015-11-30 20:00:00  2015     11   30    20       0   0.0    0.0  0.0  17.7   \n",
      "2015-11-30 21:00:00  2015     11   30    21       0   0.0    0.0  0.0  17.8   \n",
      "2015-11-30 22:00:00  2015     11   30    22       0   0.0    0.0  0.0  16.7   \n",
      "2015-11-30 23:00:00  2015     11   30    23       0   0.0    0.0  0.0  16.7   \n",
      "\n",
      "                     Tdew    RH   Pres  Wspd   Wdir  Snow Depth  \n",
      "Timestamp                                                        \n",
      "2015-11-30 19:00:00  19.6   6.0  835.4   5.3  252.0           0  \n",
      "2015-11-30 20:00:00  17.7   7.1  835.8   1.7  225.0           0  \n",
      "2015-11-30 21:00:00  17.8   7.9  836.1   2.9  190.5           0  \n",
      "2015-11-30 22:00:00  16.7   9.7  836.0   3.4  145.5           0  \n",
      "2015-11-30 23:00:00  16.7  11.7  835.9   2.9   90.0           0  \n",
      "\n",
      "Iniciando generación de TMY...\n",
      "Años disponibles en los datos: [2004, 2008, 2009, 2012, 2013, 2014, 2015]\n",
      "Calculando estadísticas mensuales y a largo plazo...\n",
      "Calculando diferencias y seleccionando meses típicos...\n",
      "  Mes 1: Año seleccionado = 2014\n",
      "  Mes 2: Año seleccionado = 2004\n",
      "  Mes 3: Año seleccionado = 2008\n",
      "  Mes 4: Año seleccionado = 2004\n",
      "  Mes 5: Año seleccionado = 2009\n",
      "  Mes 6: Año seleccionado = 2013\n",
      "  Mes 7: Año seleccionado = 2012\n",
      "  Mes 8: Año seleccionado = 2014\n",
      "  Mes 9: Año seleccionado = 2013\n",
      "  Mes 10: Año seleccionado = 2013\n",
      "  Mes 11: Año seleccionado = 2015\n",
      "  Mes 12: Año seleccionado = 2014\n",
      "\n",
      "Construyendo el DataFrame TMY final...\n",
      "TMY DataFrame construido con 8760 registros.\n",
      "Ajustando el índice de tiempo a un año genérico...\n",
      "Índice ajustado al año genérico 2000.\n",
      "Asegurando orden cronológico final del índice...\n",
      "\n",
      "Archivo TMY guardado exitosamente en: /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/antofagasta_tmy.csv\n",
      "\n",
      "Verificando orden del Timestamp en /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/antofagasta_tmy.csv...\n",
      "Verificación exitosa: El índice Timestamp está ordenado cronológicamente.\n",
      "\n",
      "Generando gráficos descriptivos del TMY final...\n",
      "  Generando gráfico de serie anual GHI (TMY) -> /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_annual_series.png...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:44: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:45: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[col].fillna(method='bfill', inplace=True)\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
      "/tmp/ipykernel_610372/846480457.py:120: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gráfico guardado como /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_annual_series.png\n",
      "  Generando histograma GHI (TMY) -> /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_ghi_histogram.png...\n",
      "    Gráfico guardado como /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_ghi_histogram.png\n",
      "  Generando matriz de correlación (TMY) -> /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_correlation_matrix.png...\n",
      "    Gráfico guardado como /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_correlation_matrix.png\n",
      "  Generando histograma GHI > 0 (TMY) -> /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_ghi_histogram_positive.png...\n",
      "    Gráfico guardado como /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_ghi_histogram_positive.png\n",
      "\n",
      "Guardando informe consolidado en: /home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/tmy_generation_report.txt...\n",
      "  Informe consolidado guardado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # Importar os\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Limpia columnas específicas del DataFrame y devuelve un log de las acciones.\"\"\"\n",
    "    print(\"Iniciando limpieza de datos...\")\n",
    "    cleaning_log = [] # Lista para guardar el resumen\n",
    "    cleaning_log.append(\"Data Cleaning Summary:\")\n",
    "    cleaning_log.append(\"=====================\")\n",
    "    \n",
    "    # Columnas a limpiar y acciones\n",
    "    cols_to_clean = {\n",
    "        'GHI': {'mark_upper_as_nan': 1250, 'interpolate': True, 'clip_lower': 0}, \n",
    "        'DNI': {'interpolate': True, 'clip_lower': 0, 'clip_upper': None}, \n",
    "        'DHI': {'interpolate': True, 'clip_lower': 0, 'clip_upper': None}, \n",
    "        'Tdry': {'interpolate': True, 'clip_lower': None, 'clip_upper': None},\n",
    "        'Wspd': {'interpolate': True, 'clip_lower': 0, 'clip_upper': None} \n",
    "    }\n",
    "\n",
    "    for col, actions in cols_to_clean.items():\n",
    "        if col in df.columns:\n",
    "            log_entry = [f\"\\nColumn: {col}\"]\n",
    "            print(f\"Limpiando {col}...\")\n",
    "            initial_nan = df[col].isnull().sum()\n",
    "            log_entry.append(f\"  Initial NaNs: {initial_nan}\")\n",
    "            marked_as_nan = 0\n",
    "\n",
    "            # 1. Marcar outliers superiores como NaN\n",
    "            upper_nan_limit = actions.get('mark_upper_as_nan')\n",
    "            if upper_nan_limit is not None:\n",
    "                outlier_mask = df[col] > upper_nan_limit\n",
    "                marked_as_nan = outlier_mask.sum()\n",
    "                if marked_as_nan > 0:\n",
    "                    df.loc[outlier_mask, col] = np.nan\n",
    "                    print(f\"  Marcados {marked_as_nan} valores > {upper_nan_limit} como NaN.\")\n",
    "                    log_entry.append(f\"  Outliers > {upper_nan_limit} marked as NaN: {marked_as_nan}\")\n",
    "\n",
    "            # 2. Interpolar NaNs\n",
    "            if actions.get('interpolate'):\n",
    "                df[col] = df[col].interpolate(method='linear')\n",
    "                df[col].fillna(method='ffill', inplace=True)\n",
    "                df[col].fillna(method='bfill', inplace=True)\n",
    "                final_nan = df[col].isnull().sum()\n",
    "                print(f\"  NaNs interpolados/rellenados (originales: {initial_nan}, marcados: {marked_as_nan}, final: {final_nan}).\")\n",
    "                log_entry.append(f\"  Interpolated/Filled NaNs (Original: {initial_nan}, Marked: {marked_as_nan}, Final: {final_nan})\")\n",
    "            \n",
    "            # 3. Clip inferior\n",
    "            lower_limit = actions.get('clip_lower')\n",
    "            if lower_limit is not None:\n",
    "                negative_mask = df[col] < lower_limit\n",
    "                negative_count = negative_mask.sum()\n",
    "                if negative_count > 0:\n",
    "                    df[col] = df[col].clip(lower=lower_limit)\n",
    "                    print(f\"  Corregidos {negative_count} valores < {lower_limit} en {col}.\")\n",
    "                    log_entry.append(f\"  Values < {lower_limit} clipped: {negative_count}\")\n",
    "            \n",
    "            cleaning_log.extend(log_entry)\n",
    "\n",
    "    print(\"Limpieza de datos completada.\")\n",
    "    cleaning_log.append(\"\\nLimpieza de datos completada.\")\n",
    "    return df, cleaning_log # Devolver DataFrame y log\n",
    "\n",
    "def calculate_fs_stat(series1, series2, num_bins=100):\n",
    "    \"\"\"Calcula la estadística Finkelstein-Schafer entre dos series.\"\"\"\n",
    "    # Calcula los CDFs empíricos\n",
    "    counts1, bin_edges = np.histogram(series1, bins=num_bins)\n",
    "    cdf1 = np.cumsum(counts1) / series1.size\n",
    "    counts2, _ = np.histogram(series2, bins=bin_edges) # Usar los mismos bins\n",
    "    cdf2 = np.cumsum(counts2) / series2.size\n",
    "    # Calcula la diferencia absoluta media (FS stat)\n",
    "    fs_stat = np.sum(np.abs(cdf1 - cdf2)) / num_bins\n",
    "    return fs_stat\n",
    "\n",
    "def generate_tmy(df):\n",
    "    \"\"\"Genera el DataFrame TMY a partir de los datos horarios limpios.\"\"\"\n",
    "    print(\"\\nIniciando generación de TMY...\")\n",
    "    \n",
    "    # Variables para la selección TMY y sus pesos\n",
    "    tmy_vars = {\n",
    "        'GHI': 0.5,\n",
    "        'DNI': 0.2, # Ponderación menor si hay más incertidumbre/NaNs originales\n",
    "        'DHI': 0.1, # Ponderación menor si hay más incertidumbre/NaNs originales\n",
    "        'Tdry': 0.2\n",
    "    }\n",
    "    \n",
    "    available_years = df.index.year.unique()\n",
    "    print(f\"Años disponibles en los datos: {available_years.tolist()}\")\n",
    "    if len(available_years) < 3:\n",
    "         print(\"Advertencia: Se recomienda tener al menos 3 años de datos para un TMY robusto.\")\n",
    "\n",
    "    monthly_stats = {}\n",
    "    long_term_stats = {}\n",
    "\n",
    "    # --- 1. Calcular estadísticas mensuales y a largo plazo --- \n",
    "    print(\"Calculando estadísticas mensuales y a largo plazo...\")\n",
    "    for month in range(1, 13):\n",
    "        long_term_monthly_data = {}\n",
    "        for var in tmy_vars:\n",
    "            long_term_monthly_data[var] = pd.Series(dtype=float)\n",
    "\n",
    "        monthly_stats[month] = {}\n",
    "        \n",
    "        for year in available_years:\n",
    "            # Seleccionar datos para el mes/año específico\n",
    "            mask = (df.index.month == month) & (df.index.year == year)\n",
    "            month_year_data = df[mask]\n",
    "            \n",
    "            if not month_year_data.empty:\n",
    "                monthly_stats[month][year] = {}\n",
    "                for var in tmy_vars:\n",
    "                    series = month_year_data[var]\n",
    "                    monthly_stats[month][year][var] = {\n",
    "                        'mean': series.mean(),\n",
    "                        'data': series # Guardamos la serie para el cálculo FS\n",
    "                    }\n",
    "                    # Acumular datos para promedio a largo plazo\n",
    "                    long_term_monthly_data[var] = pd.concat([long_term_monthly_data[var], series])\n",
    "\n",
    "        # Calcular estadísticas a largo plazo para el mes\n",
    "        long_term_stats[month] = {}\n",
    "        for var in tmy_vars:\n",
    "            series_lt = long_term_monthly_data[var]\n",
    "            long_term_stats[month][var] = {\n",
    "                'mean': series_lt.mean(),\n",
    "                'data': series_lt\n",
    "            }\n",
    "\n",
    "    # --- 2. Calcular diferencias ponderadas y seleccionar meses --- \n",
    "    print(\"Calculando diferencias y seleccionando meses típicos...\")\n",
    "    selected_months = {}\n",
    "    for month in range(1, 13):\n",
    "        best_year = None\n",
    "        min_weighted_diff = float('inf')\n",
    "        \n",
    "        if month not in monthly_stats: # Si no hay datos para este mes\n",
    "            print(f\"Advertencia: No hay datos para el mes {month}, no se puede seleccionar mes típico.\")\n",
    "            continue\n",
    "\n",
    "        for year in monthly_stats[month]:\n",
    "            total_weighted_diff = 0\n",
    "            for var, weight in tmy_vars.items():\n",
    "                # Diferencia de medias (normalizada por la media a largo plazo para evitar sesgos de escala)\n",
    "                mean_diff = abs(monthly_stats[month][year][var]['mean'] - long_term_stats[month][var]['mean'])\n",
    "                if long_term_stats[month][var]['mean'] != 0:\n",
    "                     mean_diff /= abs(long_term_stats[month][var]['mean'])\n",
    "                \n",
    "                # Diferencia de distribución (FS Stat)\n",
    "                fs_diff = calculate_fs_stat(monthly_stats[month][year][var]['data'], long_term_stats[month][var]['data'])\n",
    "\n",
    "                # Ponderar (aquí simplemente sumamos, se puede ajustar)\n",
    "                # Damos más peso a la diferencia de distribución (FS)\n",
    "                total_weighted_diff += weight * (0.3 * mean_diff + 0.7 * fs_diff) \n",
    "\n",
    "            # Actualizar si es el mejor año hasta ahora para este mes\n",
    "            if total_weighted_diff < min_weighted_diff:\n",
    "                min_weighted_diff = total_weighted_diff\n",
    "                best_year = year\n",
    "        \n",
    "        if best_year is not None:\n",
    "            selected_months[month] = best_year\n",
    "            print(f\"  Mes {month}: Año seleccionado = {best_year}\")\n",
    "        else:\n",
    "            print(f\"Advertencia: No se pudo seleccionar un año típico para el mes {month}.\")\n",
    "\n",
    "    # --- 3. Construir el DataFrame TMY --- \n",
    "    print(\"\\nConstruyendo el DataFrame TMY final...\")\n",
    "    tmy_df_list = []\n",
    "    if not selected_months:\n",
    "        print(\"Error: No se seleccionaron meses típicos. No se puede generar el TMY.\")\n",
    "        return None, None\n",
    "        \n",
    "    for month, year in selected_months.items():\n",
    "        mask = (df.index.month == month) & (df.index.year == year)\n",
    "        month_data = df[mask]\n",
    "        # Ajustar el año a un año \"típico\" (opcional, aquí usamos el año original)\n",
    "        # Podríamos crear un nuevo índice de tiempo para un año genérico si fuera necesario\n",
    "        tmy_df_list.append(month_data)\n",
    "    \n",
    "    if not tmy_df_list:\n",
    "        print(\"Error: No se pudieron recolectar datos para los meses seleccionados.\")\n",
    "        return None, None\n",
    "        \n",
    "    tmy_final_df = pd.concat(tmy_df_list).sort_index()\n",
    "    print(f\"TMY DataFrame construido con {len(tmy_final_df)} registros.\")\n",
    "\n",
    "    # --- 4. Ajustar el índice a un año genérico --- \n",
    "    print(\"Ajustando el índice de tiempo a un año genérico...\")\n",
    "    placeholder_year = 2000 # Puedes elegir otro año si prefieres\n",
    "    try:\n",
    "        # Crear nuevo índice reemplazando solo el año\n",
    "        new_index = tmy_final_df.index.map(lambda ts: ts.replace(year=placeholder_year))\n",
    "        tmy_final_df.index = new_index\n",
    "        \n",
    "        # Manejo de año bisiesto si es necesario\n",
    "        # Si el año placeholder NO es bisiesto, eliminar 29 Feb\n",
    "        if placeholder_year % 4 != 0 or (placeholder_year % 100 == 0 and placeholder_year % 400 != 0):\n",
    "             feb_29_mask = (tmy_final_df.index.month == 2) & (tmy_final_df.index.day == 29)\n",
    "             if feb_29_mask.any():\n",
    "                 print(f\"Advertencia: Año placeholder {placeholder_year} no es bisiesto. Se eliminarán {feb_29_mask.sum()} registros del 29 de Feb.\")\n",
    "                 tmy_final_df = tmy_final_df[~feb_29_mask]\n",
    "        # (Podríamos añadir lógica para duplicar un día si el placeholder ES bisiesto y el original no, pero es menos común)\n",
    "\n",
    "        # Verificar longitud final\n",
    "        if len(tmy_final_df) != 8760:\n",
    "             print(f\"Advertencia: El TMY final tiene {len(tmy_final_df)} horas, no las 8760 esperadas después del ajuste de año.\")\n",
    "        \n",
    "        print(f\"Índice ajustado al año genérico {placeholder_year}.\")\n",
    "        \n",
    "        # Actualizar las columnas Year, Month, Day, Hour para reflejar el nuevo índice\n",
    "        tmy_final_df['Year'] = tmy_final_df.index.year\n",
    "        tmy_final_df['Month'] = tmy_final_df.index.month\n",
    "        tmy_final_df['Day'] = tmy_final_df.index.day\n",
    "        tmy_final_df['Hour'] = tmy_final_df.index.hour\n",
    "        # Minute si existe y fue usado\n",
    "        if 'Minute' in tmy_final_df.columns:\n",
    "             tmy_final_df['Minute'] = tmy_final_df.index.minute\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al ajustar el índice al año genérico: {e}\")\n",
    "        # Continuar sin el ajuste si falla\n",
    "\n",
    "    # --- 5. Asegurar orden cronológico final --- \n",
    "    print(\"Asegurando orden cronológico final del índice...\")\n",
    "    tmy_final_df.sort_index(inplace=True)\n",
    "\n",
    "    # Devolver DataFrame TMY, meses seleccionados\n",
    "    return tmy_final_df, selected_months\n",
    "\n",
    "# --- Script Principal ---\n",
    "# Definir carpeta de salida con ruta absoluta\n",
    "output_dir = '/home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/' \n",
    "os.makedirs(output_dir, exist_ok=True) # Crear carpeta si no existe\n",
    "\n",
    "# Cargar el dataset\n",
    "# Usar ruta absoluta también para la carga, aunque ya funcionaba\n",
    "file_path = '/home/cparrado/datasciencesolar/datascience_solarenergy/datascience_solarenergy/antofagasta_dirty.csv'\n",
    "print(f\"Cargando datos desde: {file_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Datos cargados exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Archivo no encontrado en {file_path}\")\n",
    "    exit() # Salir si el archivo no se encuentra\n",
    "\n",
    "# Limpiar los datos y capturar el log\n",
    "df_cleaned, cleaning_summary_log = clean_data(df.copy()) \n",
    "\n",
    "# Crear Timestamp y establecer como índice\n",
    "print(\"Creando índice de tiempo (Timestamp)...\")\n",
    "try:\n",
    "    # Asegurarse de que las columnas de fecha/hora sean numéricas (a veces pueden leerse como objetos)\n",
    "    date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    for col in date_cols:\n",
    "        df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "    \n",
    "    # Eliminar filas donde la conversión falló (si alguna)\n",
    "    rows_before = len(df_cleaned)\n",
    "    df_cleaned.dropna(subset=date_cols, inplace=True)\n",
    "    if len(df_cleaned) < rows_before:\n",
    "        print(f\"Advertencia: Se eliminaron {rows_before - len(df_cleaned)} filas con valores no numéricos en columnas de fecha/hora.\")\n",
    "\n",
    "    # Convertir a enteros para asegurar compatibilidad con to_datetime\n",
    "    for col in ['Year', 'Month', 'Day', 'Hour', 'Minute']:\n",
    "         df_cleaned[col] = df_cleaned[col].astype(int)\n",
    "\n",
    "    # Crear el timestamp (ignorar 'Minute' si siempre es 0, como parece ser el caso)\n",
    "    if (df_cleaned['Minute'] == 0).all():\n",
    "        df_cleaned['Timestamp'] = pd.to_datetime(df_cleaned[['Year', 'Month', 'Day', 'Hour']])\n",
    "    else: # Si 'Minute' varía, incluirlo\n",
    "        df_cleaned['Timestamp'] = pd.to_datetime(df_cleaned[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "        \n",
    "    df_cleaned.set_index('Timestamp', inplace=True)\n",
    "    df_cleaned.sort_index(inplace=True) # Asegurar orden cronológico\n",
    "    print(\"Índice de tiempo creado y establecido.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al crear el índice de tiempo: {e}\")\n",
    "    print(\"Asegúrate de que las columnas 'Year', 'Month', 'Day', 'Hour', 'Minute' existan y sean numéricas.\")\n",
    "    exit()\n",
    "\n",
    "# Mostrar información básica del DataFrame limpio\n",
    "print(\"\\nInformación del DataFrame limpio:\")\n",
    "print(df_cleaned.info())\n",
    "print(\"\\nPrimeras filas del DataFrame limpio:\")\n",
    "print(df_cleaned.head())\n",
    "print(\"\\nÚltimas filas del DataFrame limpio:\")\n",
    "print(df_cleaned.tail())\n",
    "\n",
    "# --- Generar TMY ---\n",
    "# generate_tmy ahora solo devuelve df y selected_years\n",
    "tmy_data, selected_years = generate_tmy(df_cleaned)\n",
    "\n",
    "# --- Guardar TMY ---\n",
    "if tmy_data is not None:\n",
    "    output_file_csv = os.path.join(output_dir, 'antofagasta_tmy.csv') # Usar os.path.join\n",
    "    # output_file_csv = f\"{output_dir}/antofagasta_tmy.csv\" # Alternativa con f-string\n",
    "    try:\n",
    "        tmy_data.to_csv(output_file_csv)\n",
    "        print(f\"\\nArchivo TMY guardado exitosamente en: {output_file_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al guardar el archivo TMY: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo se generó el archivo TMY debido a errores previos.\")\n",
    "\n",
    "# --- Verificación Opcional del Orden del Índice --- \n",
    "if tmy_data is not None:\n",
    "    print(f\"\\nVerificando orden del Timestamp en {output_file_csv}...\")\n",
    "    try:\n",
    "        df_check = pd.read_csv(output_file_csv, index_col='Timestamp', parse_dates=True)\n",
    "        if df_check.index.is_monotonic_increasing:\n",
    "            print(\"Verificación exitosa: El índice Timestamp está ordenado cronológicamente.\")\n",
    "        else:\n",
    "            print(\"Error de verificación: El índice Timestamp NO está ordenado cronológicamente.\")\n",
    "            # Opcional: Mostrar dónde no está ordenado\n",
    "            # diffs = df_check.index.to_series().diff().dt.total_seconds()\n",
    "            # print(diffs[diffs < 0])\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante la verificación del archivo guardado: {e}\")\n",
    "\n",
    "# --- Generación de Gráficos del TMY Final --- \n",
    "if tmy_data is not None:\n",
    "    print(\"\\nGenerando gráficos descriptivos del TMY final...\")\n",
    "    \n",
    "    # 1. Gráfico Serie Anual GHI (TMY)\n",
    "    output_file_png_series = os.path.join(output_dir, 'tmy_annual_series.png')\n",
    "    print(f\"  Generando gráfico de serie anual GHI (TMY) -> {output_file_png_series}...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(tmy_data.index, tmy_data['GHI'])\n",
    "        plt.title(f'Serie Anual de GHI (TMY - Año {tmy_data.index.year.unique()[0]})')\n",
    "        plt.xlabel('Fecha')\n",
    "        plt.ylabel('GHI (W/m^2)')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_file_png_series)\n",
    "        plt.close()\n",
    "        print(f\"    Gráfico guardado como {output_file_png_series}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error al generar gráfico de serie anual: {e}\")\n",
    "\n",
    "    # 2. Histograma GHI (TMY)\n",
    "    output_file_png_hist = os.path.join(output_dir, 'tmy_ghi_histogram.png')\n",
    "    print(f\"  Generando histograma GHI (TMY) -> {output_file_png_hist}...\")\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(tmy_data['GHI'], kde=True, bins=50)\n",
    "        plt.title(f'Histograma de GHI (TMY - Año {tmy_data.index.year.unique()[0]})')\n",
    "        plt.xlabel('GHI (W/m^2)')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(output_file_png_hist)\n",
    "        plt.close()\n",
    "        print(f\"    Gráfico guardado como {output_file_png_hist}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error al generar histograma: {e}\")\n",
    "\n",
    "    # 3. Matriz de Correlación (TMY)\n",
    "    output_file_png_corr = os.path.join(output_dir, 'tmy_correlation_matrix.png')\n",
    "    print(f\"  Generando matriz de correlación (TMY) -> {output_file_png_corr}...\")\n",
    "    try:\n",
    "        numeric_cols_tmy = tmy_data.select_dtypes(include=np.number)\n",
    "        correlation_matrix_tmy = numeric_cols_tmy.corr()\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(correlation_matrix_tmy, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "        plt.title(f'Matriz de Correlación (TMY - Año {tmy_data.index.year.unique()[0]})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file_png_corr)\n",
    "        plt.close()\n",
    "        print(f\"    Gráfico guardado como {output_file_png_corr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error al generar matriz de correlación: {e}\")\n",
    "\n",
    "    # 4. Histograma GHI (Positivos) (TMY)\n",
    "    output_file_png_hist_pos = os.path.join(output_dir, 'tmy_ghi_histogram_positive.png')\n",
    "    print(f\"  Generando histograma GHI > 0 (TMY) -> {output_file_png_hist_pos}...\")\n",
    "    try:\n",
    "        ghi_positive = tmy_data[tmy_data['GHI'] > 0]['GHI']\n",
    "        if not ghi_positive.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(ghi_positive, kde=True, bins=50)\n",
    "            plt.title(f'Histograma de GHI > 0 (TMY - Año {tmy_data.index.year.unique()[0]})')\n",
    "            plt.xlabel('GHI (W/m^2)')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(output_file_png_hist_pos)\n",
    "            plt.close()\n",
    "            print(f\"    Gráfico guardado como {output_file_png_hist_pos}\")\n",
    "        else:\n",
    "            print(\"    No se encontraron valores de GHI > 0 para generar el histograma.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error al generar histograma de GHI positivo: {e}\")\n",
    "        \n",
    "# --- Guardar Informe Consolidado (Limpieza + Estadísticas TMY) --- \n",
    "report_file_path = os.path.join(output_dir, 'tmy_generation_report.txt')\n",
    "print(f\"\\nGuardando informe consolidado en: {report_file_path}...\")\n",
    "try:\n",
    "    with open(report_file_path, 'w') as f:\n",
    "        f.write(\"=================================================\\n\")\n",
    "        f.write(\" TMY Generation Report for Antofagasta Data \\n\")\n",
    "        f.write(\"=================================================\\n\")\n",
    "\n",
    "        # 1. Resumen de Limpieza\n",
    "        if 'cleaning_summary_log' in locals() and cleaning_summary_log:\n",
    "            f.write(\"\\n\\n--- Data Cleaning Summary ---\\n\")\n",
    "            for line in cleaning_summary_log:\n",
    "                f.write(line + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\\n--- Data Cleaning Summary ---\\n\")\n",
    "            f.write(\"No cleaning summary log available.\\n\")\n",
    "\n",
    "        # 2. Selección de Meses Típicos\n",
    "        if 'selected_years' in locals() and selected_years:\n",
    "            f.write(\"\\n\\n--- Selected Source Year for Each Month ---\\n\")\n",
    "            month_names = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "                           7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "            for month_num in sorted(selected_years.keys()):\n",
    "                f.write(f\"  {month_names.get(month_num, month_num)}: {selected_years[month_num]}\\n\")\n",
    "        else:\n",
    "             f.write(\"\\n\\n--- Selected Source Year for Each Month ---\\n\")\n",
    "             f.write(\"Month selection data not available (TMY generation might have failed).\\n\")\n",
    "\n",
    "        # 3. Estadísticas Descriptivas del TMY Final\n",
    "        if tmy_data is not None:\n",
    "            f.write(\"\\n\\n--- Descriptive Statistics of Final TMY Data ---\\n\")\n",
    "            desc_stats = tmy_data.describe()\n",
    "            f.write(desc_stats.to_string()) \n",
    "            f.write(\"\\n\")\n",
    "        else:\n",
    "             f.write(\"\\n\\n--- Descriptive Statistics of Final TMY Data ---\\n\")\n",
    "             f.write(\"TMY data not available for statistics (TMY generation might have failed).\\n\")\n",
    "\n",
    "    print(\"  Informe consolidado guardado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error al guardar el informe consolidado: {e}\")\n",
    "\n",
    "# --- Fin del Script --- \n",
    "# (Se eliminó el print final redundante) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulacion PV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
